{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install ujson\n",
    "!pip install xnetwork\n",
    "!pip install infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:02:20.039678Z",
     "iopub.status.busy": "2024-02-06T21:02:20.039410Z",
     "iopub.status.idle": "2024-02-06T21:02:21.323312Z",
     "shell.execute_reply": "2024-02-06T21:02:21.322458Z",
     "shell.execute_reply.started": "2024-02-06T21:02:20.039660Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from os.path import join as PJ\n",
    "# import bgzf\n",
    "import struct\n",
    "import numpy as np\n",
    "import operator\n",
    "import gensim\n",
    "import ujson\n",
    "import igraph as ig\n",
    "import xnetwork as xn\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from os.path import join as PJ\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:02:21.324890Z",
     "iopub.status.busy": "2024-02-06T21:02:21.324559Z",
     "iopub.status.idle": "2024-02-06T21:02:21.590526Z",
     "shell.execute_reply": "2024-02-06T21:02:21.589691Z",
     "shell.execute_reply.started": "2024-02-06T21:02:21.324868Z"
    }
   },
   "outputs": [],
   "source": [
    "from infomap import Infomap\n",
    "def infomapMembership(vertexCount,edges):\n",
    "    im = Infomap(\"-N 10 --ftree --silent --seed %d\"%np.random.randint(4294967296));\n",
    "    im.setVerbosity(0);\n",
    "    for nodeIndex in range(0,vertexCount):\n",
    "        im.add_node(nodeIndex)\n",
    "    for edge in edges:\n",
    "        im.add_link(edge[0], edge[1]);\n",
    "    im.run()\n",
    "    # print(\"Result\")\n",
    "    # print(\"\\n#node module\")\n",
    "    membership = [0]*vertexCount;\n",
    "    for node in im.tree:\n",
    "        if node.is_leaf:\n",
    "            membership[node.node_id] = node.module_id;\n",
    "    return membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:02:21.591396Z",
     "iopub.status.busy": "2024-02-06T21:02:21.591202Z",
     "iopub.status.idle": "2024-02-06T21:02:21.599321Z",
     "shell.execute_reply": "2024-02-06T21:02:21.598522Z",
     "shell.execute_reply.started": "2024-02-06T21:02:21.591378Z"
    }
   },
   "outputs": [],
   "source": [
    "def infomapApply(g, weights=None):\n",
    "    vertexCount = g.vcount()\n",
    "    if(weights):\n",
    "        edges = [(e.source, e.target, e[weights]) for e in g.es]\n",
    "    else:\n",
    "        edges = g.get_edgelist()\n",
    "\n",
    "#     if(g.is_directed()):\n",
    "#         extraOptions = \"-d\"\n",
    "#     else:\n",
    "    extraOptions = \"\"\n",
    "    im = Infomap(\"%s -N 10 --silent --seed %d\" %\n",
    "                 (extraOptions, np.random.randint(4294967296)))\n",
    "    \n",
    "    im.setVerbosity(0)\n",
    "    for nodeIndex in range(0, vertexCount):\n",
    "        im.add_node(nodeIndex)\n",
    "    for edge in edges:\n",
    "        if(len(edge) > 2):\n",
    "            if(edge[2]>0):\n",
    "                im.addLink(edge[0], edge[1], edge[2])\n",
    "            im.add_link(edge[0], edge[1], weight=edge[2])\n",
    "        else:\n",
    "            im.add_link(edge[0], edge[1])\n",
    "\n",
    "    im.run()\n",
    "    membership = [\":\".join([str(a) for a in membership])\n",
    "                  for index, membership in im.get_multilevel_modules().items()]\n",
    "\n",
    "    levelMembership = []\n",
    "    levelCount = max([len(element.split(\":\")) for element in membership])\n",
    "    for level in range(levelCount):\n",
    "        print(level)\n",
    "        levelMembership.append(\n",
    "            [\":\".join(element.split(\":\")[:(level+1)]) for element in membership]\n",
    "        )\n",
    "    return levelMembership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:02:21.601505Z",
     "iopub.status.busy": "2024-02-06T21:02:21.601256Z",
     "iopub.status.idle": "2024-02-06T21:02:21.958578Z",
     "shell.execute_reply": "2024-02-06T21:02:21.957723Z",
     "shell.execute_reply.started": "2024-02-06T21:02:21.601484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading manual dictionary an ignore list.\n",
      "Setting up nltk environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem.wordnet import WordNetLemmatizer;\n",
    "import nltk.data;\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize;\n",
    "from nltk.corpus import wordnet;\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "verboseMode = True;\n",
    "\n",
    "# Loading manual dictionary an ignore list\n",
    "if(verboseMode): print(\"Loading manual dictionary an ignore list.\");\n",
    "replaceDictionary = {};\n",
    "# with open(\"replaceDictionary.dat\",\"r\") as fp:\n",
    "# \tfor line in fp:\n",
    "# \t\tentry = line.strip().split(\"\\t\");\n",
    "# \t\tif(len(entry)>1):\n",
    "# \t\t\treplaceDictionary[entry[0]] = entry[1];\n",
    "\n",
    "# ignoreSet = set();\n",
    "# with open(\"ignoreSet.dat\",\"r\") as fp:\n",
    "# \tfor line in fp:\n",
    "# \t\tignoreSet.add(line.strip());\n",
    "\n",
    "\n",
    "#Setting up nltk environment\n",
    "if(verboseMode): print(\"Setting up nltk environment.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:02:21.959613Z",
     "iopub.status.busy": "2024-02-06T21:02:21.959404Z",
     "iopub.status.idle": "2024-02-06T21:02:22.482720Z",
     "shell.execute_reply": "2024-02-06T21:02:22.481886Z",
     "shell.execute_reply.started": "2024-02-06T21:02:21.959595Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:02:22.483554Z",
     "iopub.status.busy": "2024-02-06T21:02:22.483342Z",
     "iopub.status.idle": "2024-02-06T21:02:22.492609Z",
     "shell.execute_reply": "2024-02-06T21:02:22.491829Z",
     "shell.execute_reply.started": "2024-02-06T21:02:22.483537Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:02:22.493569Z",
     "iopub.status.busy": "2024-02-06T21:02:22.493363Z",
     "iopub.status.idle": "2024-02-06T21:02:22.566390Z",
     "shell.execute_reply": "2024-02-06T21:02:22.565583Z",
     "shell.execute_reply.started": "2024-02-06T21:02:22.493551Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:02:22.568211Z",
     "iopub.status.busy": "2024-02-06T21:02:22.567999Z",
     "iopub.status.idle": "2024-02-06T21:02:22.607218Z",
     "shell.execute_reply": "2024-02-06T21:02:22.606369Z",
     "shell.execute_reply.started": "2024-02-06T21:02:22.568192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'itself', 'against', 'been', 'being', 'own', 'of', 'hadn', 'in', 'doing', 'and', 've', 'it', 'now', \"couldn't\", 'theirs', \"haven't\", \"didn't\", 'm', 'whom', 'up', 'don', 'couldn', 'their', 'who', 'his', 'below', 'am', 'shan', \"aren't\", 'off', 't', 'haven', 'until', 'again', \"weren't\", 'that', 'didn', 'which', 'by', 'under', 'or', 'ours', 'after', 'its', 'he', 'yourselves', 'will', 'then', 'so', \"shouldn't\", 'this', 'you', 'our', 'what', 'wouldn', \"you'll\", 'can', 'll', 'myself', 'a', 'out', \"needn't\", \"won't\", 'an', 'ourselves', 'do', \"hasn't\", 'mustn', 'no', 'shouldn', \"it's\", 'had', 'down', 'above', 'only', 'where', \"you've\", 'about', 'me', 'with', 'once', 'just', 'should', \"wasn't\", 'on', \"mightn't\", 'him', 'herself', \"you'd\", 'your', 'doesn', 'weren', 'o', 'yourself', 'i', 'for', 'himself', 'not', 'during', 'through', 'such', 'further', 'nor', \"don't\", 'y', \"isn't\", 'those', 'few', 'when', 'both', 'as', 'd', 'ain', 'has', 'at', 'were', 'into', 'she', 'did', 'how', 'have', 'any', 'if', \"should've\", 'too', 'aren', 'was', 'than', 'but', 'isn', 'all', 'hasn', 'wasn', 'they', 'needn', 'we', 'most', 's', 're', 'won', 'mightn', 'my', \"mustn't\", 'some', 'from', 'these', 'the', 'very', 'there', 'between', 'each', 'be', 'more', \"she's\", \"hadn't\", 'here', \"shan't\", 'her', \"wouldn't\", 'other', 'before', 'are', 'ma', 'over', 'themselves', \"doesn't\", 'having', 'does', \"that'll\", 'is', 'hers', 'to', 'them', 'while', 'same', 'yours', \"you're\", 'why', 'because'}\n",
      "Loading manual dictionary an ignore list.\n",
      "Setting up nltk environment.\n",
      "Setting up tokenizer.\n"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem.wordnet import WordNetLemmatizer;\n",
    "import nltk.data;\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize;\n",
    "from nltk.corpus import wordnet;\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "lmtzr = WordNetLemmatizer();\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "punctuation = re.compile(r'[(\\])(\\})(\\{)(\\[).?!,\":;()|]');\n",
    "stopSet = set(stopwords.words('english'));\n",
    "\n",
    "print(stopSet)\n",
    "\n",
    "verboseMode = True;\n",
    "\n",
    "# Loading manual dictionary an ignore list\n",
    "if(verboseMode): print(\"Loading manual dictionary an ignore list.\");\n",
    "replaceDictionary = {};\n",
    "with open(\"replaceDictionary.txt\",\"r\") as fp:\n",
    "\tfor line in fp:\n",
    "\t\tentry = line.strip().split(\"\\t\");\n",
    "\t\tif(len(entry)>1):\n",
    "\t\t\treplaceDictionary[entry[0]] = entry[1];\n",
    "\n",
    "ignoreSet = set();\n",
    "with open(\"ignoreSet.txt\",\"r\") as fp:\n",
    "\tfor line in fp:\n",
    "\t\tignoreSet.add(line.strip());\n",
    "\n",
    "\n",
    "#Setting up nltk environment\n",
    "if(verboseMode): print(\"Setting up nltk environment.\");\n",
    "\n",
    "\n",
    "def findWholeWord(w):\n",
    "\treturn re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\tif treebank_tag.startswith('J'):\n",
    "\t\treturn wordnet.ADJ\n",
    "\telif treebank_tag.startswith('V'):\n",
    "\t\treturn wordnet.VERB\n",
    "\telif treebank_tag.startswith('N'):\n",
    "\t\treturn wordnet.NOUN\n",
    "\telif treebank_tag.startswith('R'):\n",
    "\t\treturn wordnet.ADV\n",
    "\telse:\n",
    "\t\treturn ''\n",
    "\n",
    "# def get_wordnet_pos(treebank_tag):\n",
    "# \tif treebank_tag.startswith('J'):\n",
    "# \t\treturn -1\n",
    "# \telif treebank_tag.startswith('V'):\n",
    "# \t\treturn -1\n",
    "# \telif treebank_tag.startswith('N'):\n",
    "# \t\treturn wordnet.NOUN\n",
    "# \telif treebank_tag.startswith('R'):\n",
    "# \t\treturn wordnet.ADV\n",
    "# \telse:\n",
    "# \t\treturn ''\n",
    "\n",
    "#Setting up tokenizer\n",
    "if(verboseMode): print(\"Setting up tokenizer.\");\n",
    "\n",
    "tokenizerInput = {\n",
    "\t\"stopSet\":stopSet,\n",
    "\t\"punctuation\":punctuation,\n",
    "\t\"tokenizer\":sent_tokenizer,\n",
    "\t\"lematizer\":lmtzr,\n",
    "\t\"sent_tokenize\": sent_tokenize,\n",
    "\t\"replaceDictionary\": replaceDictionary,\n",
    "\t\"ignoreSet\":ignoreSet\n",
    "}\n",
    "\n",
    "def tokenizeString(theString,maximumTokenSize,tokenizerInput,removeStopWords=True):\n",
    "\tstopSet = tokenizerInput[\"stopSet\"];\n",
    "\tlematizer = tokenizerInput[\"lematizer\"];\n",
    "\ttokenizer = tokenizerInput[\"tokenizer\"];\n",
    "\tpunctuation = tokenizerInput[\"punctuation\"];\n",
    "\tsent_tokenize = tokenizerInput[\"sent_tokenize\"];\n",
    "\treplaceDictionary = tokenizerInput[\"replaceDictionary\"];\n",
    "\tignoreSet = tokenizerInput[\"ignoreSet\"];\n",
    "\twordsList = [];\n",
    "\ttitleAbstract = (\". \".join(theString.split(\"::\"))).strip();\n",
    "\twordsSentences = [word_tokenize(t) for t in sent_tokenize(titleAbstract)];\n",
    "\tstopSentence = False;\n",
    "\tfor si, words in enumerate(wordsSentences):\n",
    "\t\twordsTags = nltk.pos_tag(words);\n",
    "\t\tif(stopSentence):\n",
    "\t\t\tbreak;\n",
    "\t\tfor wi,wordTag in enumerate(wordsTags):\n",
    "\t\t\tword = wordTag[0];\n",
    "\t\t\ttag = wordTag[1];\n",
    "\t\t\t\n",
    "\t\t\tif word.isdigit() or word[1:].isdigit():\n",
    "\t\t\t\tcontinue;\n",
    "# \t\t\tif(si>len(wordsSentences)-4 and (word.lower()==\"copyright\" or (wi>0 and word.lower()==\"c\" and words[wi-1] == \"(\"  and words[wi+1] == \")\" ))):\n",
    "# \t\t\t\tstopSentence = True;\n",
    "# \t\t\t\tbreak;\n",
    "\t\t\tword = punctuation.sub(\"\", word);\n",
    "\t\t\tconvTag = get_wordnet_pos(tag);\n",
    "\t\t\t#print \"w: \"+word;\n",
    "\t\t\tif convTag == -1:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif(convTag != ''):\n",
    "\t\t\t\tword  = lematizer.lemmatize(word.lower(), convTag);\n",
    "\t\t\telse:\n",
    "\t\t\t\tword  = lematizer.lemmatize(word.lower());\n",
    "\t\t\tif(len(word)==0 or ((word in stopSet) and removeStopWords) or (word in ignoreSet)):\n",
    "\t\t\t\tcontinue;\n",
    "\t\t\telse:\n",
    "\t\t\t\tif(word in replaceDictionary):\n",
    "\t\t\t\t\twordsList.append(replaceDictionary[word]);\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\twordsList.append(word);\n",
    "\n",
    "\ttokens = [set() for i in range(maximumTokenSize)];\n",
    "\tfor wordIndex in range(len(wordsList)):\n",
    "\t\tfor tokenSize in range(0,min(wordIndex+1,maximumTokenSize)):\n",
    "\t\t\ttokens[tokenSize].add(\" \".join(wordsList[(wordIndex-tokenSize):(wordIndex+1)]));\n",
    "\treturn tokens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:02:22.608940Z",
     "iopub.status.busy": "2024-02-06T21:02:22.608642Z",
     "iopub.status.idle": "2024-02-06T21:02:22.631791Z",
     "shell.execute_reply": "2024-02-06T21:02:22.630785Z",
     "shell.execute_reply.started": "2024-02-06T21:02:22.608917Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_bardosova(network, jsonFileprefix):\n",
    "    \n",
    "    removeStopWords = True;\n",
    "    maximumTokenSize = 3; #n-gram\n",
    "    minKeywordsPerCluster = 10;\n",
    "    maxKeywordsPerCluster = 10;\n",
    "    maxClusterNameLength = 150;\n",
    "    useMajorComponent = True;\n",
    "    verboseMode = True;\n",
    "    \n",
    "    # Obtaining the major connected component (if needed)\n",
    "    if(useMajorComponent):\n",
    "        if(verboseMode): print(\"Obtaining the major connected component.\");\n",
    "        network = network.clusters(\"WEAK\").giant();\n",
    "\n",
    "    # Tokenizing the abstracts\n",
    "    if(verboseMode): print(\"Tokenizing the abstracts.\");\n",
    "    tokensFrequency = [[] for i in range(maximumTokenSize)];\n",
    "    tokensGroupFrequency = [{} for i in range(maximumTokenSize)];\n",
    "\n",
    "    propertiesKeys = set();\n",
    "\n",
    "    verticesTokens = [];\n",
    "    for vertexIndex in range(network.vcount()):\n",
    "        if(vertexIndex%100==0):\n",
    "            print(\"Tokenizing: %d/%d             \"%(vertexIndex,network.vcount()),end=\"\\r\")\n",
    "\n",
    "    #         for wordsList in tokenList:\n",
    "    #             tokens = [set() for i in range(maximumTokenSize)];\n",
    "    #             for wordIndex in range(len(wordsList)):\n",
    "    #                 for tokenSize in range(0,min(wordIndex+1,maximumTokenSize)):\n",
    "    #                     tokens[tokenSize].add(\" \".join(wordsList[(wordIndex-tokenSize):(wordIndex+1)]));\n",
    "        verticesTokens.append(tokenizeString(network.vs[vertexIndex][\"title\"],maximumTokenSize,tokenizerInput));\n",
    "\n",
    "    print(\"Done                   \");\n",
    "\n",
    "    # Obtaining the network community structure\n",
    "    if(verboseMode): print(\"Obtaining the network community structure.\");\n",
    "    \n",
    "\n",
    "    edgelist = [(e.source,e.target) for e in network.es]\n",
    "    communities = infomapApply(network)[0]\n",
    "    communities = [int(c) for c in communities]\n",
    "    print()\n",
    "    \n",
    "    # print(\"Modularity: %f\"%cc.q);\n",
    "\n",
    "    clusters = [[] for i in range(max(communities)+1)];\n",
    "    for vertexIndex in range(network.vcount()):\n",
    "        clusters[communities[vertexIndex]].append(vertexIndex);\n",
    "\n",
    "    #sorting the clusters by size\n",
    "    clusters = sorted(clusters,key=len,reverse=True);\n",
    "\n",
    "    # Getting tokens frequency\n",
    "    if(verboseMode): print(\"Getting tokens frequency.\");\n",
    "\n",
    "\n",
    "    tokenFrequencyInClusters = [];\n",
    "    tokenFrequencyInCorpus = {};\n",
    "\n",
    "    for clusterIndex in range(len(clusters)):\n",
    "        cluster = clusters[clusterIndex];\n",
    "        tokenFrequencyInCluster = {};\n",
    "        for vertexIndex in cluster:\n",
    "            tokens = verticesTokens[vertexIndex];\n",
    "            for tokenSize in range(0,maximumTokenSize):\n",
    "                for token in tokens[tokenSize]:\n",
    "                    if(token not in tokenFrequencyInCorpus):\n",
    "                        tokenFrequencyInCorpus[token] = 0;\n",
    "                    if(token not in tokenFrequencyInCluster):\n",
    "                        tokenFrequencyInCluster[token] = 0;\n",
    "                    tokenFrequencyInCorpus[token] += 1;\n",
    "                    tokenFrequencyInCluster[token] += 1;\n",
    "        tokenFrequencyInClusters.append(tokenFrequencyInCluster);\n",
    "\n",
    "    # Calculating the importance Index\n",
    "    if(verboseMode): print(\"Calculating the importance Index.\");\n",
    "    #tokenRelativeFrequencyInClusters = [];\n",
    "    #tokenRelativeFrequencyOutClusters = [];\n",
    "    tokenImportanceIndexInClusters = [];\n",
    "\n",
    "    verticesCount = network.vcount();\n",
    "    for clusterIndex in range(len(clusters)):\n",
    "        clusterSize = len(clusters[clusterIndex]);\n",
    "\n",
    "        tokenFrequencyInCluster = tokenFrequencyInClusters[clusterIndex];\n",
    "\n",
    "        #tokenRelativeFrequencyInCluster = {};\n",
    "        #tokenRelativeFrequencyOutCluster = {};\n",
    "        tokenImportanceIndexInCluster = {};\n",
    "\n",
    "        for token in tokenFrequencyInCluster:\n",
    "            nInCluster = tokenFrequencyInCluster[token];\n",
    "            nOutCluster = tokenFrequencyInCorpus[token]-nInCluster;\n",
    "            outClusterSize = verticesCount-clusterSize;\n",
    "            if(nOutCluster==0):\n",
    "                outClusterSize = 1; #Fix for singletons\n",
    "            FInCluster = float(nInCluster)/float(clusterSize);\n",
    "            FOutCluster = float(nOutCluster)/float(outClusterSize);\n",
    "            importanceIndex = FInCluster-FOutCluster;\n",
    "            #tokenRelativeFrequencyInCluster[token] = FInCluster;\n",
    "            #tokenRelativeFrequencyOutCluster[token] = FOutCluster;\n",
    "            tokenImportanceIndexInCluster[token] = importanceIndex;\n",
    "\n",
    "        #tokenRelativeFrequencyInClusters.append(tokenRelativeFrequencyInCluster);\n",
    "        #tokenRelativeFrequencyOutClusters.append(tokenRelativeFrequencyOutCluster);\n",
    "        tokenImportanceIndexInClusters.append(tokenImportanceIndexInCluster);\n",
    "\n",
    "    defaultNames = \"ABCDEFGHIJKLMNOPQRSTUWVXYZ\";\n",
    "    defaultNamesLength = len(defaultNames);\n",
    "\n",
    "    clusterKeywords = [];\n",
    "    minClusterSize = min([len(cluster) for cluster in clusters]);\n",
    "    maxClusterSize = max([len(cluster) for cluster in clusters]);\n",
    "    clusterNames = [];\n",
    "    for clusterIndex in range(len(clusters)):\n",
    "        cluster = clusters[clusterIndex];\n",
    "        clusterSize = len(cluster);\n",
    "        keywords = [v[0] for v in sorted(tokenImportanceIndexInClusters[clusterIndex].items(),key=operator.itemgetter(1),reverse=True)];\n",
    "        if(maxClusterSize>minClusterSize):\n",
    "            m = (maxKeywordsPerCluster-minKeywordsPerCluster)/float(maxClusterSize-minClusterSize);\n",
    "        else:\n",
    "            m=0;\n",
    "        keywordsCount = round(m*(clusterSize-minClusterSize)+minKeywordsPerCluster);\n",
    "        currentKeywords = [];\n",
    "        while(len(currentKeywords)<keywordsCount and len(keywords)>len(currentKeywords)):\n",
    "            currentKeywords = keywords[0:keywordsCount];\n",
    "            jointKeywords = \".\"+\".\".join(currentKeywords)+\".\";\n",
    "            toRemoveKeywords = [];\n",
    "            for keyword in currentKeywords:\n",
    "                if(jointKeywords.find(\" %s.\"%keyword)>=0):\n",
    "                    toRemoveKeywords.append(keyword);\n",
    "                elif(jointKeywords.find(\".%s \"%keyword)>=0):\n",
    "                    toRemoveKeywords.append(keyword);\n",
    "            for toRemoveKeyword in toRemoveKeywords:\n",
    "                keywords.remove(toRemoveKeyword);\n",
    "                currentKeywords.remove(toRemoveKeyword);\n",
    "        clusterKeywords.append(currentKeywords);\n",
    "        #print(currentKeywords);\n",
    "        clusterName = \"\";\n",
    "        if(clusterIndex<defaultNamesLength):\n",
    "            clusterName += defaultNames[clusterIndex];\n",
    "        else:\n",
    "            clusterName += \"{%d}\"%(clusterIndex);\n",
    "        clusterName += \" - \"+\", \".join(currentKeywords);\n",
    "        if(len(clusterName)>maxClusterNameLength):\n",
    "            clusterName = clusterName[0:maxClusterNameLength-1]+\"...\";\n",
    "        for vertexIndex in cluster:\n",
    "            network.vs[vertexIndex][\"Cluster Name\"] = clusterName;\n",
    "            network.vs[vertexIndex][\"Cluster Index\"] = clusterIndex;\n",
    "        clusterNames.append(clusterName);\n",
    "        print(clusterName);\n",
    "\n",
    "\n",
    "    # Saving the network\n",
    "    if(verboseMode): print(\"Saving the network.\");\n",
    "    # network.vs[\"kcore\"] = network.coreness()\n",
    "\n",
    "    xn.igraph2xnet(network,fileName=PJ('',\"%s_infomap.xnet\"%(jsonFileprefix)),ignoredNodeAtts=[\"Text\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T23:56:38.492793Z",
     "iopub.status.busy": "2024-02-06T23:56:38.492505Z",
     "iopub.status.idle": "2024-02-06T23:56:38.996239Z",
     "shell.execute_reply": "2024-02-06T23:56:38.995492Z",
     "shell.execute_reply.started": "2024-02-06T23:56:38.492770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Passive Learning Sensor Architecture for Multimodal Image Labeling: An Application for Social Robots', 'Data-Driven Condition Monitoring of Mining Mobile Machinery in Non-Stationary Operations Using Wireless Accelerometer Sensor Modules', 'Deep Learning Based Prediction Towards Designing A Smart Building Assistant System', 'Deep Person Detection in Two-Dimensional Range Data', 'Machine Vision for UAS Ground Operations', 'A deep learning based secured energy management framework within a smart island', 'Automatic Video Editing for Sensor-Rich Videos', 'Using Touchscreen Interaction Data to Predict Cognitive Workload', 'Surface EMG vs. High-Density EMG: Tradeoff Between Performance and Usability for Head Orientation Prediction in VR Application', 'Sniff Species: SURMOF-Based Sensor Array Discriminates Aromatic Plants beyond the Genus Level']\n"
     ]
    }
   ],
   "source": [
    "# file = 'cit_sensos_network.xnet'\n",
    "file = 'cit_sensos_network_daniel_06_02.xnet'\n",
    "network = xn.xnet2igraph(file)\n",
    "network.vs['wos_id'] = network.vs['name']\n",
    "network.vs['name'] = network.vs['title']\n",
    "\n",
    "print(network.vs['name'][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T00:00:32.858956Z",
     "iopub.status.busy": "2024-02-07T00:00:32.858665Z",
     "iopub.status.idle": "2024-02-07T00:00:37.311214Z",
     "shell.execute_reply": "2024-02-07T00:00:37.310260Z",
     "shell.execute_reply.started": "2024-02-07T00:00:32.858937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining the major connected component.\n",
      "Tokenizing the abstracts.\n",
      "Done                              \n",
      "Obtaining the network community structure.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "\n",
      "Getting tokens frequency.\n",
      "Calculating the importance Index.\n",
      "A - gait, human activity recognition, wearable sensor, fall detection, classification, parkinson 's disease, inertial sensor, recognition using, deep...\n",
      "B - electronic nose, machine learning, low-cost, air quality, gas, pollution, sensor array, artificial, sensing, chemical\n",
      "C - data, plant, image, imagery, crop, machine, hyperspectral, review, remote sensing, yield\n",
      "D - fault diagnosis, deep, tool, convolutional, monitoring, useful life, predictive, remain, neural network, base\n",
      "E - wireless sensor network, machine learning, anomaly detection, iot, detection wireless sensor, attack, internet thing, intrusion detection system,...\n",
      "F - structural health monitoring, damage, displacement, bridge, identification, structure, video, computer vision, dynamic, vibration\n",
      "G - pressure sensor, flexible, tactile, sensor base, wearable, soft, strain sensor, skin, human, triboelectric\n",
      "H - deep learning, application, fault, industrial process, quality, soft sensor modeling, prediction, base deep, treatment, wastewater\n",
      "I - stress detection, physiological, machine learning, driver, passive, mental health, depression, wearable sensor, using wearable, smartphone\n",
      "J - thermal comfort, energy, wearable, building, eating, intake, activity, smart, consumption, review\n",
      "K - system, healthcare, smart, internet thing, framework, disease, big, real-time, monitoring, health\n",
      "L - iot, deep learning, data, internet thing, traffic speed prediction, machine learning, traffic flow, network, short-term, application\n",
      "M - sensor, occupancy detection, smart, building, indoor, tracking, occupancy prediction, infrared, thermal, energy\n",
      "N - system, technology, using, monitoring, flash flood, blood pressure, machine learning, non-invasive, internet, urban\n",
      "O - event-based, tactile, neuromorphic, camera, grasp, classification, event, vision sensor, object, hand\n",
      "P - weld, control, quality, sensor fault, cell, classification, laser welding, predictive maintenance, deep neural network, monitoring\n",
      "Q - lidar, 3d object detection, fusion, point cloud, deep learning, survey, autonomous driving, drone, autonomous vehicle, image\n",
      "R - drone, device, user, autonomous, wearable, racing, mobile, wrist-worn, gesture, localization\n",
      "S - smart, system, health, covid-19, monitoring, autonomous vehicle, cognitive, dementia, deep, surveillance\n",
      "T - parkinson 's disease, motor, sensor network, system, device, multivariate, smart, time series, anomaly detection, survey\n",
      "U - fire, analysis, kinect, depth, posture, evaluation, exercise recognition, rehabilitation exercise, fall, sit\n",
      "W - activity recognition, human, diabetes mellitus, sleep, blood glucose, review, mining, continuous, segmentation, multimodal\n",
      "V - autism spectrum disorder, deep, stereotypical motor movement, detection, anomaly, child autism spectrum, virtual reality, identify, series, behav...\n",
      "X - water distribution network, leak, machine, contamination source identification, predictive control, fault detection, data, learning-based, proces...\n",
      "Y - atrial fibrillation, signal, detection, assessment, cardiac, cardiovascular, heart, non-invasive, wearable, feature\n",
      "Z - \n",
      "Saving the network.\n"
     ]
    }
   ],
   "source": [
    "output_header = 'cit_sensors_bardosova_daniel_06_02'\n",
    "apply_bardosova(network, output_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:02:42.405614Z",
     "iopub.status.busy": "2024-02-06T21:02:42.405409Z",
     "iopub.status.idle": "2024-02-06T21:02:42.769324Z",
     "shell.execute_reply": "2024-02-06T21:02:42.768460Z",
     "shell.execute_reply.started": "2024-02-06T21:02:42.405596Z"
    }
   },
   "outputs": [],
   "source": [
    "net_bardosova = xn.xnet2igraph('cit_sensors_bardosova_infomap.xnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:02:42.770789Z",
     "iopub.status.busy": "2024-02-06T21:02:42.770593Z",
     "iopub.status.idle": "2024-02-06T21:02:42.776042Z",
     "shell.execute_reply": "2024-02-06T21:02:42.775022Z",
     "shell.execute_reply.started": "2024-02-06T21:02:42.770771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'Cluster Index', 'Cluster Name', 'abstract', 'title', 'wos_id']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_bardosova.vs.attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T07:09:49.092529Z",
     "iopub.status.busy": "2024-01-19T07:09:49.092288Z",
     "iopub.status.idle": "2024-01-19T07:09:49.099275Z",
     "shell.execute_reply": "2024-01-19T07:09:49.098375Z",
     "shell.execute_reply.started": "2024-01-19T07:09:49.092512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A - data, machine learning, remote sense, forest, mapping, map, remote sensing, hyperspectral, soil, image',\n",
       " 'B - bupivacaine, ropivacaine, surgery, effect, nerve, comparison, epidural, brachial plexus block, spinal, anesthesia',\n",
       " 'C - learning, wireless sensor network, iot, spectrum, data, privacy, internet, privacy-preserving, attack, thing',\n",
       " 'D - human activity recognition, fall detection, smartphone, system, wearable sensor, smart, mobile, deep, inertial, device',\n",
       " 'E - artificial intelligence, image, diagnosis, cancer, deep learning, breast, diabetic retinopathy, detection, neural, convolutional',\n",
       " 'F - mechanoluminescence, tactile, flexible, wearable, phosphor, pressure sensor, mechanoluminescent, material, soft, triboelectric',\n",
       " 'G - disease, diabetes, diagnosis, breast cancer, prediction, system, feature, classification, machine, model',\n",
       " \"H - parkinson 's disease, inertial sensor, wearable sensor, machine learning, classification, assessment, freezing gait, movement, recognition, patie...\",\n",
       " 'I - monitoring, machine, system, tool, predictive maintenance, process, deep, fault, learning, sensor',\n",
       " 'J - feature, driver, machine, glioblastoma, behavior, brain, drive, glioma, interface, analysis',\n",
       " 'K - covid-19, chest, image, ct, diagnosis, artificial intelligence, deep, patient, x-ray, pneumonia',\n",
       " 'L - electronic nose, base, drift, machine, sensor array, gas sensor, e-nose, chemical, compensation, quality',\n",
       " 'M - stress, depression, wearable, disorder, mobile, mental health, predict, machine learning, smartphone, suicide',\n",
       " 'N - eeg, signal, pain, disorder, seizure prediction, classification, seizure detection, epileptic seizure, feature, major depressive',\n",
       " 'O - circulate tumor cell, cancer cell, cancer patient, peripheral blood, lung cancer, detection circulate tumor, isolation, capture, colorectal, brea...',\n",
       " 'P - predict, machine learn, sepsis, patient, blood, prediction, early, asthma, care, rapid',\n",
       " 'Q - systematic review, machine, classification, fatty liver disease, preterm birth, fault diagnosis, iot, artificial, accurate, movement',\n",
       " 'R - predict, prediction, patient, adrenal incidentalomas, machine learning, model, cancer, kidney, analysis, machine learn',\n",
       " 'S - detection, pain, sensor, electrochemical, analysis, machine, quantification, sample, mobile, nanopore',\n",
       " 'T - sensor, air quality, low-cost, monitoring, air pollution, smart, calibration, system, machine, iot',\n",
       " 'U - atrial fibrillation, artificial intelligence, cardiac, electrocardiogram, detection, ventricular, detect, algorithm, arrhythmia, ecg',\n",
       " 'V - learn, drug sensitivity prediction, drug response prediction, cancer cell, serum, machine, prediction cancer, microrna target prediction, improve...',\n",
       " 'W - structural health monitoring, damage detection, structure, machine, cloud, identification, bridge, learning, data-driven, sensor',\n",
       " 'X - brain, multiple sclerosis, predict, functional, fmri, detection, classifier, machine, risk, psychosis',\n",
       " 'Y - estimation, maximum likelihood, wireless sensor network, array, signal, source localization, localization wireless sensor, noise, target, doa',\n",
       " 'Z - predict fluid responsiveness, cardiac, surgery, index, stroke volume variation, fluid responsiveness patient, challenge, pulse pressure variation...',\n",
       " '{26} - learning, machine, acoustic, fiber, sensor, pipeline, deep, optic, data, sensing',\n",
       " '{27} - machine learning, support vector machine, predict, machine learn, non-invasive, fuzzy, rheumatoid arthritis, glucose, risk, patient',\n",
       " '{28} - sensor, building, smart, energy, system, base, indoor, occupancy detection, iot, infrared',\n",
       " '{29} - radiomics, image, cell lung cancer, model, non-small cell lung, pet/ct, metastasis, machine learn, change detection, patient',\n",
       " \"{30} - signal, machine learning, fetal, prediction, heart rate, coronary artery, assessment, parkinson 's disease, 's, myocardial perfusion\",\n",
       " '{31} - process, fault detection, artificial intelligence, photovoltaic, application, reference evapotranspiration, soft sensor, china, aridity index,...',\n",
       " \"{32} - alzheimer 's disease, diagnosis, mild cognitive impairment, functional, brain, classification, machine learning, support, schizophrenia, struc...\",\n",
       " '{33} - word sense disambiguation, autism spectrum disorder, sense, identify, child autism, biomedical, functional, clinical, screen, abbreviation',\n",
       " '{34} - radar, system, human activity recognition, device, gesture recognition, environment, autonomous vehicle, iot, survey, sign',\n",
       " '{35} - system, platform, internet thing, mobile crowdsensing, smart city, traffic, framework, big data, fake, index modulation',\n",
       " '{36} - cancer, detection, cell-free dna, early, prediction, discovery, colorectal, artificial intelligence, mycobacterium tuberculosis, methylation',\n",
       " '{37} - obstructive sleep apnea, fairness, bias, model, classification, learn, fair, explainable, ecg, adult',\n",
       " '{38} - expansion, stroke, acute, intracerebral hemorrhage, spot, predict, machine learn, hematoma, receptor, sign',\n",
       " '{39} - liver, benign, malignant, contrast-enhanced ultrasound, radiomics, lesion, ct, renal cell carcinoma, differentiation, cell',\n",
       " '{40} - wireless sensor network, intrusion detection, artificial intelligence, rout, technique, malware detection, protocol, attack, android, allocati...',\n",
       " '{41} - dysphagia, emotion, recognition, wearable, mobile, swallow, food, intake, swallowing, egocentric',\n",
       " '{42} - escherichia coli, detection salmonella, assay, rapid, food, water, real-time pcr, amplification, milk, sensitive',\n",
       " '{43} - detection, base, electrochemical, biosensor, oxide, immunosensor, graphene, cancer biomarker, carbon, nanostructured',\n",
       " '{44} - primary auditory cortex, neuron, cortical, representation, response, cat primary auditory-cortex, stimulus, sound, field, sensitivity',\n",
       " '{45} - prediction, radiomics, breast, analysis, raman spectroscopy, ct, feature, machine learn, machine learning, pancreatic ductal',\n",
       " '{46} - survey, representation, cognitive, model, sense turing test, quest real ai, test quest real, common sense turing, turing test quest, symbol',\n",
       " '{47} - predict, patient, machine learn, hospital readmission, prediction, risk, machine learning, model, heart failure, acute',\n",
       " '{48} - smart home, cognitive, activity, intelligent, base, research, care, artificial intelligence, automate, health',\n",
       " '{49} - machine learning, covid-19, medical, early, classification, iot, concussion, leukemia, postural control, injury',\n",
       " '{50} - personal thermal comfort, thermal comfort model, building, temperature, control, thermal sensation, individual thermal, occupant, skin, predict',\n",
       " '{51} - explainable, population, classification, deep learning, detect, sweep, ecg signal, soft, deep neural network, analysis',\n",
       " '{52} - platelet concentrate, photochemical treatment, rapid sensitive, water sample, detection escherichia coli, detect, bacterial contamination, amo...',\n",
       " '{53} - cerebral, imaging, ct, volume, patient, infarct, acute ischemic stroke, diffusion-weighted, core, malignant',\n",
       " '{54} - iot, data, site, base, edge, deep learning, prediction, intelligence, malonylation, compression',\n",
       " '{55} - biopsy, radiomics, histoscanning tm, multiparametric mri, significant prostate cancer, clinically significant prostate, radical prostatectomy,...',\n",
       " '{56} - sensor, gesture recognition, base, machine learning, hand, force myography, rehabilitation system, glove, recognition wearable, young',\n",
       " '{57} - variant, coronary heart disease, genomic, somatic mutation, next-generation, industry, dna, disorder, tumor, artificial intelligence',\n",
       " '{58} - patient, record, electronic, machine learn, systemic, alcohol, identify, classifier, health, machine learning'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(net_bardosova.vs['Cluster Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
