{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install ujson\n",
    "!pip install xnetwork\n",
    "!pip install infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:31:36.716268Z",
     "iopub.status.busy": "2024-02-08T15:31:36.715563Z",
     "iopub.status.idle": "2024-02-08T15:31:37.656901Z",
     "shell.execute_reply": "2024-02-08T15:31:37.656036Z",
     "shell.execute_reply.started": "2024-02-08T15:31:36.716227Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from os.path import join as PJ\n",
    "# import bgzf\n",
    "import struct\n",
    "import numpy as np\n",
    "import operator\n",
    "import gensim\n",
    "import ujson\n",
    "import igraph as ig\n",
    "import xnetwork as xn\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from os.path import join as PJ\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:31:37.658692Z",
     "iopub.status.busy": "2024-02-08T15:31:37.658008Z",
     "iopub.status.idle": "2024-02-08T15:31:37.849655Z",
     "shell.execute_reply": "2024-02-08T15:31:37.848773Z",
     "shell.execute_reply.started": "2024-02-08T15:31:37.658672Z"
    }
   },
   "outputs": [],
   "source": [
    "from infomap import Infomap\n",
    "def infomapMembership(vertexCount,edges):\n",
    "    im = Infomap(\"-N 10 --ftree --silent --seed %d\"%np.random.randint(4294967296));\n",
    "    im.setVerbosity(0);\n",
    "    for nodeIndex in range(0,vertexCount):\n",
    "        im.add_node(nodeIndex)\n",
    "    for edge in edges:\n",
    "        im.add_link(edge[0], edge[1]);\n",
    "    im.run()\n",
    "    # print(\"Result\")\n",
    "    # print(\"\\n#node module\")\n",
    "    membership = [0]*vertexCount;\n",
    "    for node in im.tree:\n",
    "        if node.is_leaf:\n",
    "            membership[node.node_id] = node.module_id;\n",
    "    return membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:31:37.850978Z",
     "iopub.status.busy": "2024-02-08T15:31:37.850774Z",
     "iopub.status.idle": "2024-02-08T15:31:37.858061Z",
     "shell.execute_reply": "2024-02-08T15:31:37.857284Z",
     "shell.execute_reply.started": "2024-02-08T15:31:37.850959Z"
    }
   },
   "outputs": [],
   "source": [
    "def infomapApply(g, weights=None):\n",
    "    vertexCount = g.vcount()\n",
    "    if(weights):\n",
    "        edges = [(e.source, e.target, e[weights]) for e in g.es]\n",
    "    else:\n",
    "        edges = g.get_edgelist()\n",
    "\n",
    "#     if(g.is_directed()):\n",
    "#         extraOptions = \"-d\"\n",
    "#     else:\n",
    "    extraOptions = \"\"\n",
    "    im = Infomap(\"%s -N 10 --silent --seed %d\" %\n",
    "                 (extraOptions, np.random.randint(4294967296)))\n",
    "    \n",
    "    im.setVerbosity(0)\n",
    "    for nodeIndex in range(0, vertexCount):\n",
    "        im.add_node(nodeIndex)\n",
    "    for edge in edges:\n",
    "        if(len(edge) > 2):\n",
    "            if(edge[2]>0):\n",
    "                im.addLink(edge[0], edge[1], edge[2])\n",
    "            im.add_link(edge[0], edge[1], weight=edge[2])\n",
    "        else:\n",
    "            im.add_link(edge[0], edge[1])\n",
    "\n",
    "    im.run()\n",
    "    membership = [\":\".join([str(a) for a in membership])\n",
    "                  for index, membership in im.get_multilevel_modules().items()]\n",
    "\n",
    "    levelMembership = []\n",
    "    levelCount = max([len(element.split(\":\")) for element in membership])\n",
    "    for level in range(levelCount):\n",
    "        print(level)\n",
    "        levelMembership.append(\n",
    "            [\":\".join(element.split(\":\")[:(level+1)]) for element in membership]\n",
    "        )\n",
    "    return levelMembership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:31:38.190456Z",
     "iopub.status.busy": "2024-02-08T15:31:38.190191Z",
     "iopub.status.idle": "2024-02-08T15:31:38.498082Z",
     "shell.execute_reply": "2024-02-08T15:31:38.497451Z",
     "shell.execute_reply.started": "2024-02-08T15:31:38.190437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading manual dictionary an ignore list.\n",
      "Setting up nltk environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem.wordnet import WordNetLemmatizer;\n",
    "import nltk.data;\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize;\n",
    "from nltk.corpus import wordnet;\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "verboseMode = True;\n",
    "\n",
    "# Loading manual dictionary an ignore list\n",
    "if(verboseMode): print(\"Loading manual dictionary an ignore list.\");\n",
    "replaceDictionary = {};\n",
    "# with open(\"replaceDictionary.dat\",\"r\") as fp:\n",
    "# \tfor line in fp:\n",
    "# \t\tentry = line.strip().split(\"\\t\");\n",
    "# \t\tif(len(entry)>1):\n",
    "# \t\t\treplaceDictionary[entry[0]] = entry[1];\n",
    "\n",
    "# ignoreSet = set();\n",
    "# with open(\"ignoreSet.dat\",\"r\") as fp:\n",
    "# \tfor line in fp:\n",
    "# \t\tignoreSet.add(line.strip());\n",
    "\n",
    "\n",
    "#Setting up nltk environment\n",
    "if(verboseMode): print(\"Setting up nltk environment.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:31:38.601042Z",
     "iopub.status.busy": "2024-02-08T15:31:38.600763Z",
     "iopub.status.idle": "2024-02-08T15:31:39.010613Z",
     "shell.execute_reply": "2024-02-08T15:31:39.009700Z",
     "shell.execute_reply.started": "2024-02-08T15:31:38.601023Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:31:39.023402Z",
     "iopub.status.busy": "2024-02-08T15:31:39.023133Z",
     "iopub.status.idle": "2024-02-08T15:31:39.032669Z",
     "shell.execute_reply": "2024-02-08T15:31:39.031739Z",
     "shell.execute_reply.started": "2024-02-08T15:31:39.023382Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:31:39.387443Z",
     "iopub.status.busy": "2024-02-08T15:31:39.387168Z",
     "iopub.status.idle": "2024-02-08T15:31:39.469689Z",
     "shell.execute_reply": "2024-02-08T15:31:39.468982Z",
     "shell.execute_reply.started": "2024-02-08T15:31:39.387423Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:31:39.825721Z",
     "iopub.status.busy": "2024-02-08T15:31:39.825474Z",
     "iopub.status.idle": "2024-02-08T15:31:39.844906Z",
     "shell.execute_reply": "2024-02-08T15:31:39.844021Z",
     "shell.execute_reply.started": "2024-02-08T15:31:39.825703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'just', 'they', 'wouldn', 'each', 'what', 'do', 'because', 'our', 'he', 'into', 'any', 'very', \"you'll\", 'against', 'which', 'ain', 'down', 'who', 'over', 'so', 'am', 'those', 'haven', 'them', 'isn', 'the', 'off', 'once', 'your', 'yourself', 'too', \"wouldn't\", \"won't\", 'nor', 'before', 'only', 'further', 'some', 'wasn', \"needn't\", \"it's\", 'is', 't', 'did', 'when', 'its', 'while', 'no', 'hers', 'does', 'him', 'out', 'now', \"she's\", 'most', 'how', 'myself', 'hadn', 'are', 'shan', \"couldn't\", 'ourselves', 'don', 'd', 'below', 'being', 'not', 're', 'you', 'after', 'then', 'itself', 'where', 'their', 'above', 'until', 'his', 'we', \"don't\", 'yourselves', 'other', 's', 'this', 'under', 'both', 'i', 'y', 'there', 'weren', 'in', 'all', 'having', 'whom', 'with', 'for', 'should', 'mustn', 'at', \"didn't\", \"you're\", 'have', 'about', \"mightn't\", 'needn', 'been', 'will', 'has', 'she', 'theirs', 'it', 'my', 'here', 'on', \"you've\", 'yours', \"hadn't\", 'more', 'a', \"doesn't\", \"shouldn't\", 'ours', 'hasn', 'himself', 'through', 'herself', 'or', 'than', 'll', \"you'd\", 'ma', \"wasn't\", \"haven't\", 'o', 'me', \"mustn't\", \"hasn't\", 'during', 'to', 've', 'were', 'be', 'own', 'of', 'as', 'same', 'won', 'shouldn', 'couldn', \"should've\", 'doing', \"that'll\", 'that', 'again', 'by', 'up', 'aren', 'her', 'and', 'from', 'an', 'was', 'm', 'if', \"shan't\", \"aren't\", 'doesn', 'why', 'these', 'can', \"isn't\", 'mightn', \"weren't\", 'between', 'but', 'few', 'themselves', 'such', 'didn', 'had'}\n",
      "Loading manual dictionary an ignore list.\n",
      "Setting up nltk environment.\n",
      "Setting up tokenizer.\n"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem.wordnet import WordNetLemmatizer;\n",
    "import nltk.data;\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize;\n",
    "from nltk.corpus import wordnet;\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "lmtzr = WordNetLemmatizer();\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "punctuation = re.compile(r'[(\\])(\\})(\\{)(\\[).?!,\":;()|]');\n",
    "stopSet = set(stopwords.words('english'));\n",
    "\n",
    "print(stopSet)\n",
    "\n",
    "verboseMode = True;\n",
    "\n",
    "# Loading manual dictionary an ignore list\n",
    "if(verboseMode): print(\"Loading manual dictionary an ignore list.\");\n",
    "replaceDictionary = {};\n",
    "with open(\"replaceDictionary.txt\",\"r\") as fp:\n",
    "\tfor line in fp:\n",
    "\t\tentry = line.strip().split(\"\\t\");\n",
    "\t\tif(len(entry)>1):\n",
    "\t\t\treplaceDictionary[entry[0]] = entry[1];\n",
    "\n",
    "ignoreSet = set();\n",
    "with open(\"ignoreSet.txt\",\"r\") as fp:\n",
    "\tfor line in fp:\n",
    "\t\tignoreSet.add(line.strip());\n",
    "\n",
    "\n",
    "#Setting up nltk environment\n",
    "if(verboseMode): print(\"Setting up nltk environment.\");\n",
    "\n",
    "\n",
    "def findWholeWord(w):\n",
    "\treturn re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\tif treebank_tag.startswith('J'):\n",
    "\t\treturn wordnet.ADJ\n",
    "\telif treebank_tag.startswith('V'):\n",
    "\t\treturn wordnet.VERB\n",
    "\telif treebank_tag.startswith('N'):\n",
    "\t\treturn wordnet.NOUN\n",
    "\telif treebank_tag.startswith('R'):\n",
    "\t\treturn wordnet.ADV\n",
    "\telse:\n",
    "\t\treturn ''\n",
    "\n",
    "# def get_wordnet_pos(treebank_tag):\n",
    "# \tif treebank_tag.startswith('J'):\n",
    "# \t\treturn -1\n",
    "# \telif treebank_tag.startswith('V'):\n",
    "# \t\treturn -1\n",
    "# \telif treebank_tag.startswith('N'):\n",
    "# \t\treturn wordnet.NOUN\n",
    "# \telif treebank_tag.startswith('R'):\n",
    "# \t\treturn wordnet.ADV\n",
    "# \telse:\n",
    "# \t\treturn ''\n",
    "\n",
    "#Setting up tokenizer\n",
    "if(verboseMode): print(\"Setting up tokenizer.\");\n",
    "\n",
    "tokenizerInput = {\n",
    "\t\"stopSet\":stopSet,\n",
    "\t\"punctuation\":punctuation,\n",
    "\t\"tokenizer\":sent_tokenizer,\n",
    "\t\"lematizer\":lmtzr,\n",
    "\t\"sent_tokenize\": sent_tokenize,\n",
    "\t\"replaceDictionary\": replaceDictionary,\n",
    "\t\"ignoreSet\":ignoreSet\n",
    "}\n",
    "\n",
    "def tokenizeString(theString,maximumTokenSize,tokenizerInput,removeStopWords=True):\n",
    "\tstopSet = tokenizerInput[\"stopSet\"];\n",
    "\tlematizer = tokenizerInput[\"lematizer\"];\n",
    "\ttokenizer = tokenizerInput[\"tokenizer\"];\n",
    "\tpunctuation = tokenizerInput[\"punctuation\"];\n",
    "\tsent_tokenize = tokenizerInput[\"sent_tokenize\"];\n",
    "\treplaceDictionary = tokenizerInput[\"replaceDictionary\"];\n",
    "\tignoreSet = tokenizerInput[\"ignoreSet\"];\n",
    "\twordsList = [];\n",
    "\ttitleAbstract = (\". \".join(theString.split(\"::\"))).strip();\n",
    "\twordsSentences = [word_tokenize(t) for t in sent_tokenize(titleAbstract)];\n",
    "\tstopSentence = False;\n",
    "\tfor si, words in enumerate(wordsSentences):\n",
    "\t\twordsTags = nltk.pos_tag(words);\n",
    "\t\tif(stopSentence):\n",
    "\t\t\tbreak;\n",
    "\t\tfor wi,wordTag in enumerate(wordsTags):\n",
    "\t\t\tword = wordTag[0];\n",
    "\t\t\ttag = wordTag[1];\n",
    "\t\t\t\n",
    "\t\t\tif word.isdigit() or word[1:].isdigit():\n",
    "\t\t\t\tcontinue;\n",
    "# \t\t\tif(si>len(wordsSentences)-4 and (word.lower()==\"copyright\" or (wi>0 and word.lower()==\"c\" and words[wi-1] == \"(\"  and words[wi+1] == \")\" ))):\n",
    "# \t\t\t\tstopSentence = True;\n",
    "# \t\t\t\tbreak;\n",
    "\t\t\tword = punctuation.sub(\"\", word);\n",
    "\t\t\tconvTag = get_wordnet_pos(tag);\n",
    "\t\t\t#print \"w: \"+word;\n",
    "\t\t\tif convTag == -1:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif(convTag != ''):\n",
    "\t\t\t\tword  = lematizer.lemmatize(word.lower(), convTag);\n",
    "\t\t\telse:\n",
    "\t\t\t\tword  = lematizer.lemmatize(word.lower());\n",
    "\t\t\tif(len(word)==0 or ((word in stopSet) and removeStopWords) or (word in ignoreSet)):\n",
    "\t\t\t\tcontinue;\n",
    "\t\t\telse:\n",
    "\t\t\t\tif(word in replaceDictionary):\n",
    "\t\t\t\t\twordsList.append(replaceDictionary[word]);\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\twordsList.append(word);\n",
    "\n",
    "\ttokens = [set() for i in range(maximumTokenSize)];\n",
    "\tfor wordIndex in range(len(wordsList)):\n",
    "\t\tfor tokenSize in range(0,min(wordIndex+1,maximumTokenSize)):\n",
    "\t\t\ttokens[tokenSize].add(\" \".join(wordsList[(wordIndex-tokenSize):(wordIndex+1)]));\n",
    "\treturn tokens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:31:40.437638Z",
     "iopub.status.busy": "2024-02-08T15:31:40.437372Z",
     "iopub.status.idle": "2024-02-08T15:31:40.453652Z",
     "shell.execute_reply": "2024-02-08T15:31:40.452736Z",
     "shell.execute_reply.started": "2024-02-08T15:31:40.437620Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_bardosova(network, jsonFileprefix):\n",
    "    \n",
    "    removeStopWords = True;\n",
    "    maximumTokenSize = 3; #n-gram\n",
    "    minKeywordsPerCluster = 10;\n",
    "    maxKeywordsPerCluster = 10;\n",
    "    maxClusterNameLength = 150;\n",
    "    useMajorComponent = True;\n",
    "    verboseMode = True;\n",
    "    \n",
    "    # Obtaining the major connected component (if needed)\n",
    "    if(useMajorComponent):\n",
    "        if(verboseMode): print(\"Obtaining the major connected component.\");\n",
    "        network = network.clusters(\"WEAK\").giant();\n",
    "\n",
    "    # Tokenizing the abstracts\n",
    "    if(verboseMode): print(\"Tokenizing the abstracts.\");\n",
    "    tokensFrequency = [[] for i in range(maximumTokenSize)];\n",
    "    tokensGroupFrequency = [{} for i in range(maximumTokenSize)];\n",
    "\n",
    "    propertiesKeys = set();\n",
    "\n",
    "    verticesTokens = [];\n",
    "    for vertexIndex in range(network.vcount()):\n",
    "        if(vertexIndex%100==0):\n",
    "            print(\"Tokenizing: %d/%d             \"%(vertexIndex,network.vcount()),end=\"\\r\")\n",
    "\n",
    "    #         for wordsList in tokenList:\n",
    "    #             tokens = [set() for i in range(maximumTokenSize)];\n",
    "    #             for wordIndex in range(len(wordsList)):\n",
    "    #                 for tokenSize in range(0,min(wordIndex+1,maximumTokenSize)):\n",
    "    #                     tokens[tokenSize].add(\" \".join(wordsList[(wordIndex-tokenSize):(wordIndex+1)]));\n",
    "        verticesTokens.append(tokenizeString(network.vs[vertexIndex][\"title\"],maximumTokenSize,tokenizerInput));\n",
    "\n",
    "    print(\"Done                   \");\n",
    "\n",
    "    # Obtaining the network community structure\n",
    "    if(verboseMode): print(\"Obtaining the network community structure.\");\n",
    "    \n",
    "\n",
    "    edgelist = [(e.source,e.target) for e in network.es]\n",
    "    communities = infomapApply(network)[0]\n",
    "    communities = [int(c) for c in communities]\n",
    "    print()\n",
    "    \n",
    "    # print(\"Modularity: %f\"%cc.q);\n",
    "\n",
    "    clusters = [[] for i in range(max(communities)+1)];\n",
    "    for vertexIndex in range(network.vcount()):\n",
    "        clusters[communities[vertexIndex]].append(vertexIndex);\n",
    "\n",
    "    #sorting the clusters by size\n",
    "    clusters = sorted(clusters,key=len,reverse=True);\n",
    "\n",
    "    # Getting tokens frequency\n",
    "    if(verboseMode): print(\"Getting tokens frequency.\");\n",
    "\n",
    "\n",
    "    tokenFrequencyInClusters = [];\n",
    "    tokenFrequencyInCorpus = {};\n",
    "\n",
    "    for clusterIndex in range(len(clusters)):\n",
    "        cluster = clusters[clusterIndex];\n",
    "        tokenFrequencyInCluster = {};\n",
    "        for vertexIndex in cluster:\n",
    "            tokens = verticesTokens[vertexIndex];\n",
    "            for tokenSize in range(0,maximumTokenSize):\n",
    "                for token in tokens[tokenSize]:\n",
    "                    if(token not in tokenFrequencyInCorpus):\n",
    "                        tokenFrequencyInCorpus[token] = 0;\n",
    "                    if(token not in tokenFrequencyInCluster):\n",
    "                        tokenFrequencyInCluster[token] = 0;\n",
    "                    tokenFrequencyInCorpus[token] += 1;\n",
    "                    tokenFrequencyInCluster[token] += 1;\n",
    "        tokenFrequencyInClusters.append(tokenFrequencyInCluster);\n",
    "\n",
    "    # Calculating the importance Index\n",
    "    if(verboseMode): print(\"Calculating the importance Index.\");\n",
    "    #tokenRelativeFrequencyInClusters = [];\n",
    "    #tokenRelativeFrequencyOutClusters = [];\n",
    "    tokenImportanceIndexInClusters = [];\n",
    "\n",
    "    verticesCount = network.vcount();\n",
    "    for clusterIndex in range(len(clusters)):\n",
    "        clusterSize = len(clusters[clusterIndex]);\n",
    "\n",
    "        tokenFrequencyInCluster = tokenFrequencyInClusters[clusterIndex];\n",
    "\n",
    "        #tokenRelativeFrequencyInCluster = {};\n",
    "        #tokenRelativeFrequencyOutCluster = {};\n",
    "        tokenImportanceIndexInCluster = {};\n",
    "\n",
    "        for token in tokenFrequencyInCluster:\n",
    "            nInCluster = tokenFrequencyInCluster[token];\n",
    "            nOutCluster = tokenFrequencyInCorpus[token]-nInCluster;\n",
    "            outClusterSize = verticesCount-clusterSize;\n",
    "            if(nOutCluster==0):\n",
    "                outClusterSize = 1; #Fix for singletons\n",
    "            FInCluster = float(nInCluster)/float(clusterSize);\n",
    "            FOutCluster = float(nOutCluster)/float(outClusterSize);\n",
    "            importanceIndex = FInCluster-FOutCluster;\n",
    "            #tokenRelativeFrequencyInCluster[token] = FInCluster;\n",
    "            #tokenRelativeFrequencyOutCluster[token] = FOutCluster;\n",
    "            tokenImportanceIndexInCluster[token] = importanceIndex;\n",
    "\n",
    "        #tokenRelativeFrequencyInClusters.append(tokenRelativeFrequencyInCluster);\n",
    "        #tokenRelativeFrequencyOutClusters.append(tokenRelativeFrequencyOutCluster);\n",
    "        tokenImportanceIndexInClusters.append(tokenImportanceIndexInCluster);\n",
    "\n",
    "    defaultNames = \"ABCDEFGHIJKLMNOPQRSTUWVXYZ\";\n",
    "    defaultNamesLength = len(defaultNames);\n",
    "\n",
    "    clusterKeywords = [];\n",
    "    minClusterSize = min([len(cluster) for cluster in clusters]);\n",
    "    maxClusterSize = max([len(cluster) for cluster in clusters]);\n",
    "    clusterNames = [];\n",
    "    for clusterIndex in range(len(clusters)):\n",
    "        cluster = clusters[clusterIndex];\n",
    "        clusterSize = len(cluster);\n",
    "        keywords = [v[0] for v in sorted(tokenImportanceIndexInClusters[clusterIndex].items(),key=operator.itemgetter(1),reverse=True)];\n",
    "        if(maxClusterSize>minClusterSize):\n",
    "            m = (maxKeywordsPerCluster-minKeywordsPerCluster)/float(maxClusterSize-minClusterSize);\n",
    "        else:\n",
    "            m=0;\n",
    "        keywordsCount = round(m*(clusterSize-minClusterSize)+minKeywordsPerCluster);\n",
    "        currentKeywords = [];\n",
    "        while(len(currentKeywords)<keywordsCount and len(keywords)>len(currentKeywords)):\n",
    "            currentKeywords = keywords[0:keywordsCount];\n",
    "            jointKeywords = \".\"+\".\".join(currentKeywords)+\".\";\n",
    "            toRemoveKeywords = [];\n",
    "            for keyword in currentKeywords:\n",
    "                if(jointKeywords.find(\" %s.\"%keyword)>=0):\n",
    "                    toRemoveKeywords.append(keyword);\n",
    "                elif(jointKeywords.find(\".%s \"%keyword)>=0):\n",
    "                    toRemoveKeywords.append(keyword);\n",
    "            for toRemoveKeyword in toRemoveKeywords:\n",
    "                keywords.remove(toRemoveKeyword);\n",
    "                currentKeywords.remove(toRemoveKeyword);\n",
    "        clusterKeywords.append(currentKeywords);\n",
    "        #print(currentKeywords);\n",
    "        clusterName = \"\";\n",
    "        if(clusterIndex<defaultNamesLength):\n",
    "            clusterName += defaultNames[clusterIndex];\n",
    "        else:\n",
    "            clusterName += \"{%d}\"%(clusterIndex);\n",
    "        clusterName += \" - \"+\", \".join(currentKeywords);\n",
    "        if(len(clusterName)>maxClusterNameLength):\n",
    "            clusterName = clusterName[0:maxClusterNameLength-1]+\"...\";\n",
    "        for vertexIndex in cluster:\n",
    "            network.vs[vertexIndex][\"Cluster Name\"] = clusterName;\n",
    "            network.vs[vertexIndex][\"Cluster Index\"] = clusterIndex;\n",
    "        clusterNames.append(clusterName);\n",
    "        print(clusterName);\n",
    "\n",
    "\n",
    "    # Saving the network\n",
    "    if(verboseMode): print(\"Saving the network.\");\n",
    "    # network.vs[\"kcore\"] = network.coreness()\n",
    "\n",
    "    xn.igraph2xnet(network,fileName=PJ('',\"%s_infomap.xnet\"%(jsonFileprefix)),ignoredNodeAtts=[\"Text\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:31:41.124646Z",
     "iopub.status.busy": "2024-02-08T15:31:41.124379Z",
     "iopub.status.idle": "2024-02-08T15:31:42.230745Z",
     "shell.execute_reply": "2024-02-08T15:31:42.229807Z",
     "shell.execute_reply.started": "2024-02-08T15:31:41.124628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Protein bodies in monomolecular film - Measurements according to Nouy and Langmuir', 'A study of antigens and antibodies by the monolayer film technique of Langmuir', 'Mrs. Blodgett Provides Living Endowment', 'The Blodgett Readers', 'The Blodgett Fifth Reader', 'The Blodgett Fourth Reader; Wade and Sylvester Fifth Reader; Buckwalter Fourth Reader', 'The structure of Langmuir-Blodgett films of stearic acid', 'Blodgett Readers. A Third Reader', 'The surface tension and the tangental pressure in the capillary layer in connection with the osmotic pressure in the film theory of Pockels, Langmuir and Adam.', 'A REPETITION OF THE BLODGETT EXPERIMENT ON LATENT LEARNING']\n"
     ]
    }
   ],
   "source": [
    "file = 'cit_lang_mono_film_network_luan_08_02.xnet'\n",
    "network = xn.xnet2igraph(file)\n",
    "network.vs['wos_id'] = network.vs['name']\n",
    "network.vs['name'] = network.vs['title']\n",
    "\n",
    "print(network.vs['name'][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:33:20.086812Z",
     "iopub.status.busy": "2024-02-08T15:33:20.086527Z",
     "iopub.status.idle": "2024-02-08T15:33:54.809418Z",
     "shell.execute_reply": "2024-02-08T15:33:54.808604Z",
     "shell.execute_reply.started": "2024-02-08T15:33:20.086793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining the major connected component.\n",
      "Tokenizing the abstracts.\n",
      "Done                                \n",
      "Obtaining the network community structure.\n",
      "0\n",
      "1\n",
      "2\n",
      "\n",
      "Getting tokens frequency.\n",
      "Calculating the importance Index.\n",
      "A - langmuir-blodgett film, interface, monolayer, langmuir monolayers, molecular, langmuir-blodgett-films, structure, interaction, lipid, langmuir fi...\n",
      "B - removal, adsorption, aqueous solution, dye, ii, carbon, adsorbent, equilibrium, ion, kinetic\n",
      "C - mild steel, corrosion inhibitor, acid, solution, corrosion inhibition, hcl, medium, carbon steel, experimental, extract\n",
      "D - acid, film aqueous solution, situ infrared spectroscopic, onto, tio2 aqueous solution, lysine, spectroscopic adsorption, ion, mechanism, surface\n",
      "E - adsorption, sorption, cellulose mechanism binding, mechanism binding molecular, binding molecular recognition, dormant, aspergillus niger, spore,...\n",
      "F - desorption kinetic constant, geometric determine adsorption, adsorption desorption kinetic, determine adsorption desorption, interaction biotin g...\n",
      "G - generate sol-gel process, silicate generate sol-gel, template silicate generate, polysaccharide template silicate, chitosan biosorbent removal, i...\n",
      "H - \n",
      "Saving the network.\n"
     ]
    }
   ],
   "source": [
    "output_header = 'cit_lang_mono_film_network_luan_08_02'\n",
    "apply_bardosova(network, output_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:34:17.787175Z",
     "iopub.status.busy": "2024-02-08T15:34:17.786898Z",
     "iopub.status.idle": "2024-02-08T15:34:18.915274Z",
     "shell.execute_reply": "2024-02-08T15:34:18.914417Z",
     "shell.execute_reply.started": "2024-02-08T15:34:17.787156Z"
    }
   },
   "outputs": [],
   "source": [
    "net_bardosova = xn.xnet2igraph('cit_lang_mono_film_network_luan_08_02_infomap.xnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:34:20.052993Z",
     "iopub.status.busy": "2024-02-08T15:34:20.052643Z",
     "iopub.status.idle": "2024-02-08T15:34:20.058737Z",
     "shell.execute_reply": "2024-02-08T15:34:20.057970Z",
     "shell.execute_reply.started": "2024-02-08T15:34:20.052969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'Cluster Index', 'Cluster Name', 'abstract', 'title', 'wos_id']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_bardosova.vs.attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-08T15:32:18.600840Z",
     "iopub.status.idle": "2024-02-08T15:32:18.601113Z",
     "shell.execute_reply": "2024-02-08T15:32:18.600997Z",
     "shell.execute_reply.started": "2024-02-08T15:32:18.600984Z"
    }
   },
   "outputs": [],
   "source": [
    "set(net_bardosova.vs['Cluster Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
